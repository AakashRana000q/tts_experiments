{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f2173bd-a059-41b8-b041-2a7bf89851bc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Skywork/Skywork-o1-Open-PRM-Qwen-2.5-1.5B/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Skywork--Skywork-o1-Open-PRM-Qwen-2.5-1.5B/snapshots/faf7ee3519ba8883bc951645a9a65b262d630ace/config.json\n",
      "Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForPrmModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"Skywork/Skywork-o1-Open-PRM-Qwen-2.5-1.5B--configuration_qwen2_rm.Qwen2RMConfig\",\n",
      "    \"AutoModel\": \"Skywork/Skywork-o1-Open-PRM-Qwen-2.5-1.5B--modeling_qwen2_rm.Qwen2ForRewardModel\"\n",
      "  },\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"end_token_id\": 151645,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Skywork/Skywork-o1-Open-PRM-Qwen-2.5-1.5B/resolve/main/model.safetensors HTTP/1.1\" 404 0\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--Skywork--Skywork-o1-Open-PRM-Qwen-2.5-1.5B/snapshots/faf7ee3519ba8883bc951645a9a65b262d630ace/pytorch_model.bin\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"pad_token_id\": 151643\n",
      "}\n",
      "\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/Skywork/Skywork-o1-Open-PRM-Qwen-2.5-1.5B HTTP/1.1\" 200 4567\n",
      "Attempting to create safetensors variant\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/Skywork/Skywork-o1-Open-PRM-Qwen-2.5-1.5B/commits/main HTTP/1.1\" 200 2858\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/Skywork/Skywork-o1-Open-PRM-Qwen-2.5-1.5B/discussions?p=0 HTTP/1.1\" 200 673\n",
      "Attempting to convert .bin model on the fly to safetensors.\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): safetensors-convert.hf.space:443\n",
      "DEBUG:urllib3.connectionpool:https://safetensors-convert.hf.space:443 \"POST /call/run HTTP/1.1\" 200 47\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): safetensors-convert.hf.space:443\n",
      "DEBUG:urllib3.connectionpool:https://safetensors-convert.hf.space:443 \"GET /call/run/e8ab76760a414a7d9f90ea67ef0e838d HTTP/1.1\" 200 None\n",
      "Spawning safetensors automatic conversion.\n",
      "Safetensors conversion status: complete\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/Skywork/Skywork-o1-Open-PRM-Qwen-2.5-1.5B/commits/main HTTP/1.1\" 200 2858\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/Skywork/Skywork-o1-Open-PRM-Qwen-2.5-1.5B/discussions?p=0 HTTP/1.1\" 200 673\n",
      "Some weights of the model checkpoint at Skywork/Skywork-o1-Open-PRM-Qwen-2.5-1.5B were not used when initializing Qwen2ForCausalLM: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing Qwen2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Qwen2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Skywork/Skywork-o1-Open-PRM-Qwen-2.5-1.5B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: huggingface.co\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Skywork/Skywork-o1-Open-PRM-Qwen-2.5-1.5B/resolve/main/generation_config.json HTTP/1.1\" 404 0\n",
      "Generation config file not found, using a generation config created from the model config.\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Skywork/Skywork-o1-Open-PRM-Qwen-2.5-1.5B/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Skywork--Skywork-o1-Open-PRM-Qwen-2.5-1.5B/snapshots/faf7ee3519ba8883bc951645a9a65b262d630ace/vocab.json\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Skywork--Skywork-o1-Open-PRM-Qwen-2.5-1.5B/snapshots/faf7ee3519ba8883bc951645a9a65b262d630ace/merges.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Skywork--Skywork-o1-Open-PRM-Qwen-2.5-1.5B/snapshots/faf7ee3519ba8883bc951645a9a65b262d630ace/tokenizer.json\n",
      "loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--Skywork--Skywork-o1-Open-PRM-Qwen-2.5-1.5B/snapshots/faf7ee3519ba8883bc951645a9a65b262d630ace/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--Skywork--Skywork-o1-Open-PRM-Qwen-2.5-1.5B/snapshots/faf7ee3519ba8883bc951645a9a65b262d630ace/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Skywork--Skywork-o1-Open-PRM-Qwen-2.5-1.5B/snapshots/faf7ee3519ba8883bc951645a9a65b262d630ace/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Skywork/Skywork-o1-Open-PRM-Qwen-2.5-1.5B\").eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Skywork/Skywork-o1-Open-PRM-Qwen-2.5-1.5B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dad07d22-61d3-4b11-b611-074d05e2eabe",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m input_ids, steps, reward_flags = \u001b[38;5;28mzip\u001b[39m(*processed_data)\n\u001b[32m     20\u001b[39m input_ids, attention_mask, reward_flags = prepare_batch_input_for_model(input_ids, reward_flags, tokenizer.pad_token_id)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m _, _, rewards = model(input_ids=input_ids, attention_mask=attention_mask, return_probs=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     22\u001b[39m step_rewards = derive_step_rewards(rewards, reward_flags)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mstep_rewards:\u001b[39m\u001b[33m\"\u001b[39m,step_rewards[\u001b[32m0\u001b[39m])\n",
      "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from model_utils.prm_model import PRM_MODEL\n",
    "from model_utils.io_utils import prepare_input, prepare_batch_input_for_model, derive_step_rewards\n",
    "\n",
    "datas = [\n",
    "    {\n",
    "        \"problem\"  : \"Janet\\u2019s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\",\n",
    "        \"response\" : \"To determine how much money Janet makes every day at the farmers' market, we need to follow these steps:\\n1. **Calculate the total number of eggs laid by the ducks per day.**\\n   Janet's ducks lay 16 eggs per day.\\n2. **Determine the number of eggs Janet uses each day.**\\n   - She eats 3 eggs for breakfast every morning.\\n   - She bakes muffins for her friends every day with 4 eggs.\\n   So, the total number of eggs used per day is:\\n   \\\\[\\n   3 + 4 = 7 \\\\text{ eggs}\\n   \\\\]\\n3. **Calculate the number of eggs Janet sells at the farmers' market each day.**\\n   Subtract the number of eggs used from the total number of eggs laid:\\n   \\\\[\\n   16 - 7 = 9 \\\\text{ eggs}\\n   \\\\]\\n4. **Determine how much money Janet makes from selling the eggs.**\\n   She sells each egg for $2, so the total amount of money she makes is:\\n   \\\\[\\n   9 \\\\times 2 = 18 \\\\text{ dollars}\\n   \\\\]\\nTherefore, the amount of money Janet makes every day at the farmers' market is $\\\\boxed{18}$.\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\"  : \"Janet\\u2019s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\",\n",
    "        \"response\" : \"To find out how much Janet makes every day at the farmers' market, we need to follow these steps:\\n1. Calculate the total number of eggs laid by the ducks: \\n   - Janet's ducks lay 16 eggs per day.\\n2. Calculate the total number of eggs consumed by Janet and used for baking:\\n   - Janet eats 3 eggs for breakfast.\\n   - She bakes 4 eggs for muffins.\\n   - Total eggs used: 3 (breakfast) + 4 (baking) = 7 eggs.\\n3. Calculate the remaining number of eggs for sale:\\n   - Total eggs laid: 16\\n   - Eggs used: 7\\n   - Remaining eggs: 16 - 7 = 9 eggs\\n4. Calculate the total amount of money made at the farmers' market:\\n   - Price per egg: $2\\n   - Number of eggs sold: 9\\n   - Total money made: 9 * $2 = $18\\nTherefore, Janet makes $\\\\boxed{18}$ dollars every day at the farmers' market.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "processed_data = [prepare_input(d[\"problem\"], d[\"response\"], tokenizer=tokenizer, step_token=\"\\n\") for d in datas]\n",
    "input_ids, steps, reward_flags = zip(*processed_data)\n",
    "\n",
    "input_ids, attention_mask, reward_flags = prepare_batch_input_for_model(input_ids, reward_flags, tokenizer.pad_token_id)\n",
    "_, _, rewards = model(input_ids=input_ids, attention_mask=attention_mask, return_probs=True)\n",
    "step_rewards = derive_step_rewards(rewards, reward_flags)\n",
    "print(\"step_rewards:\",step_rewards[0])\n",
    "print(\"step_rewards:\",step_rewards[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8aa6d454-7a53-47b4-a50d-18f7bc3b31f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join(\"/workspace\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e78cca1f-ffe0-48b8-a1f1-2cecaa17bf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=\"DEBUG\")\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"]=\"0\"\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"]=\"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9987acb0-73d6-474c-9181-d5ca47c7bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.logging.set_verbosity_debug()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e1f8a2f-e77f-4ebd-8e96-dab8a9ead30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd503d78-299a-4d4a-8b9a-422449722f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA H100 80GB HBM3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should output \"True\"\n",
    "print(torch.cuda.device_count())  # Should show ≥1 GPU\n",
    "print(torch.cuda.get_device_name(0))  # Check GPU model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "822660e4-6241-4779-b578-e7515ad82a65",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodel_utils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprm_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PRM_MODEL\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodel_utils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m prepare_input, prepare_batch_input_for_model, derive_step_rewards\n\u001b[32m      5\u001b[39m prm_model_path = \u001b[33m\"\u001b[39m\u001b[33mworkspace/models/Skywork-o1-Open-PRM-Qwen-2.5-7B\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'model_utils'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from model_utils.prm_model import PRM_MODEL\n",
    "from model_utils.io_utils import prepare_input, prepare_batch_input_for_model, derive_step_rewards\n",
    "\n",
    "prm_model_path = \"workspace/models/Skywork-o1-Open-PRM-Qwen-2.5-7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(prm_model_path, trust_remote_code=True)\n",
    "# datas = [\n",
    "#     {\n",
    "#         \"problem\"  : \"Janet\\u2019s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\",\n",
    "#         \"response\" : \"To determine how much money Janet makes every day at the farmers' market, we need to follow these steps:\\n1. **Calculate the total number of eggs laid by the ducks per day.**\\n   Janet's ducks lay 16 eggs per day.\\n2. **Determine the number of eggs Janet uses each day.**\\n   - She eats 3 eggs for breakfast every morning.\\n   - She bakes muffins for her friends every day with 4 eggs.\\n   So, the total number of eggs used per day is:\\n   \\\\[\\n   3 + 4 = 7 \\\\text{ eggs}\\n   \\\\]\\n3. **Calculate the number of eggs Janet sells at the farmers' market each day.**\\n   Subtract the number of eggs used from the total number of eggs laid:\\n   \\\\[\\n   16 - 7 = 9 \\\\text{ eggs}\\n   \\\\]\\n4. **Determine how much money Janet makes from selling the eggs.**\\n   She sells each egg for $2, so the total amount of money she makes is:\\n   \\\\[\\n   9 \\\\times 2 = 18 \\\\text{ dollars}\\n   \\\\]\\nTherefore, the amount of money Janet makes every day at the farmers' market is $\\\\boxed{18}$.\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"problem\"  : \"Janet\\u2019s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\",\n",
    "#         \"response\" : \"To find out how much Janet makes every day at the farmers' market, we need to follow these steps:\\n1. Calculate the total number of eggs laid by the ducks: \\n   - Janet's ducks lay 16 eggs per day.\\n2. Calculate the total number of eggs consumed by Janet and used for baking:\\n   - Janet eats 3 eggs for breakfast.\\n   - She bakes 4 eggs for muffins.\\n   - Total eggs used: 3 (breakfast) + 4 (baking) = 7 eggs.\\n3. Calculate the remaining number of eggs for sale:\\n   - Total eggs laid: 16\\n   - Eggs used: 7\\n   - Remaining eggs: 16 - 7 = 9 eggs\\n4. Calculate the total amount of money made at the farmers' market:\\n   - Price per egg: $2\\n   - Number of eggs sold: 9\\n   - Total money made: 9 * $2 = $18\\nTherefore, Janet makes $\\\\boxed{18}$ dollars every day at the farmers' market.\"\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "\n",
    "# processed_data = [prepare_input(d[\"problem\"], d[\"response\"], tokenizer=tokenizer, step_token=\"\\n\") for d in datas]\n",
    "# input_ids, steps, reward_flags = zip(*processed_data)\n",
    "\n",
    "model = PRM_MODEL.from_pretrained(prm_model_path, device_map=\"auto\").eval()\n",
    "# input_ids, attention_mask, reward_flags = prepare_batch_input_for_model(input_ids, reward_flags, tokenizer.pad_token_id)\n",
    "# _, _, rewards = model(input_ids=input_ids, attention_mask=attention_mask, return_probs=True)\n",
    "# step_rewards = derive_step_rewards(rewards, reward_flags)\n",
    "# print(\"step_rewards:\",step_rewards[0])\n",
    "# print(\"step_rewards:\",step_rewards[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380b95a9-3ac5-4aca-8f63-3fe887ea576b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Skywork/Skywork-o1-Open-PRM-Qwen-2.5-7B/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:filelock:Attempting to acquire lock 129340284404176 on /root/.cache/huggingface/hub/.locks/models--Skywork--Skywork-o1-Open-PRM-Qwen-2.5-7B/28f7e81ba175116475c3e50b7d0cebf35a47d87f.lock\n",
      "DEBUG:filelock:Lock 129340284404176 acquired on /root/.cache/huggingface/hub/.locks/models--Skywork--Skywork-o1-Open-PRM-Qwen-2.5-7B/28f7e81ba175116475c3e50b7d0cebf35a47d87f.lock\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /Skywork/Skywork-o1-Open-PRM-Qwen-2.5-7B/resolve/main/config.json HTTP/1.1\" 200 851\n",
      "DEBUG:filelock:Attempting to release lock 129340284404176 on /root/.cache/huggingface/hub/.locks/models--Skywork--Skywork-o1-Open-PRM-Qwen-2.5-7B/28f7e81ba175116475c3e50b7d0cebf35a47d87f.lock\n",
      "DEBUG:filelock:Lock 129340284404176 released on /root/.cache/huggingface/hub/.locks/models--Skywork--Skywork-o1-Open-PRM-Qwen-2.5-7B/28f7e81ba175116475c3e50b7d0cebf35a47d87f.lock\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Skywork--Skywork-o1-Open-PRM-Qwen-2.5-7B/snapshots/c3f1e8065a3ca88cd65dd2f05665af0843d29e21/config.json\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Skywork/Skywork-o1-Open-PRM-Qwen-2.5-7B/resolve/main/configuration_qwen2_rm.py HTTP/1.1\" 200 0\n",
      "DEBUG:filelock:Attempting to acquire lock 129340284617360 on /root/.cache/huggingface/hub/.locks/models--Skywork--Skywork-o1-Open-PRM-Qwen-2.5-7B/2e1afe69e783a11c22b212e54d1f422fe1e5d4a4.lock\n",
      "DEBUG:filelock:Lock 129340284617360 acquired on /root/.cache/huggingface/hub/.locks/models--Skywork--Skywork-o1-Open-PRM-Qwen-2.5-7B/2e1afe69e783a11c22b212e54d1f422fe1e5d4a4.lock\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /Skywork/Skywork-o1-Open-PRM-Qwen-2.5-7B/resolve/main/configuration_qwen2_rm.py HTTP/1.1\" 200 6678\n",
      "DEBUG:filelock:Attempting to release lock 129340284617360 on /root/.cache/huggingface/hub/.locks/models--Skywork--Skywork-o1-Open-PRM-Qwen-2.5-7B/2e1afe69e783a11c22b212e54d1f422fe1e5d4a4.lock\n",
      "DEBUG:filelock:Lock 129340284617360 released on /root/.cache/huggingface/hub/.locks/models--Skywork--Skywork-o1-Open-PRM-Qwen-2.5-7B/2e1afe69e783a11c22b212e54d1f422fe1e5d4a4.lock\n",
      "A new version of the following files was downloaded from https://huggingface.co/Skywork/Skywork-o1-Open-PRM-Qwen-2.5-7B:\n",
      "- configuration_qwen2_rm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Skywork--Skywork-o1-Open-PRM-Qwen-2.5-7B/snapshots/c3f1e8065a3ca88cd65dd2f05665af0843d29e21/config.json\n",
      "Model config Qwen2RMConfig {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForPrmModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"Skywork/Skywork-o1-Open-PRM-Qwen-2.5-7B--configuration_qwen2_rm.Qwen2RMConfig\",\n",
      "    \"AutoModel\": \"Skywork/Skywork-o1-Open-PRM-Qwen-2.5-7B--modeling_qwen2_rm.Qwen2ForRewardModel\"\n",
      "  },\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"end_token_id\": 151645,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\n",
      "}\n",
      "\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Skywork/Skywork-o1-Open-PRM-Qwen-2.5-7B/resolve/main/modeling_qwen2_rm.py HTTP/1.1\" 200 0\n",
      "DEBUG:filelock:Attempting to acquire lock 129337767885008 on /root/.cache/huggingface/hub/.locks/models--Skywork--Skywork-o1-Open-PRM-Qwen-2.5-7B/5b0b71280439dac95925b3139d0b343b54ffc61d.lock\n",
      "DEBUG:filelock:Lock 129337767885008 acquired on /root/.cache/huggingface/hub/.locks/models--Skywork--Skywork-o1-Open-PRM-Qwen-2.5-7B/5b0b71280439dac95925b3139d0b343b54ffc61d.lock\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /Skywork/Skywork-o1-Open-PRM-Qwen-2.5-7B/resolve/main/modeling_qwen2_rm.py HTTP/1.1\" 200 72741\n",
      "DEBUG:filelock:Attempting to release lock 129337767885008 on /root/.cache/huggingface/hub/.locks/models--Skywork--Skywork-o1-Open-PRM-Qwen-2.5-7B/5b0b71280439dac95925b3139d0b343b54ffc61d.lock\n",
      "DEBUG:filelock:Lock 129337767885008 released on /root/.cache/huggingface/hub/.locks/models--Skywork--Skywork-o1-Open-PRM-Qwen-2.5-7B/5b0b71280439dac95925b3139d0b343b54ffc61d.lock\n",
      "A new version of the following files was downloaded from https://huggingface.co/Skywork/Skywork-o1-Open-PRM-Qwen-2.5-7B:\n",
      "- modeling_qwen2_rm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Skywork/Skywork-o1-Open-PRM-Qwen-2.5-7B/resolve/main/pytorch_model.bin HTTP/1.1\" 302 0\n",
      "DEBUG:filelock:Attempting to acquire lock 129337767927120 on /root/.cache/huggingface/hub/.locks/models--Skywork--Skywork-o1-Open-PRM-Qwen-2.5-7B/39ebec613eb83f95e974925db2d9ec4dd6d3799885b546e8d1af9345cc722d4d.lock\n",
      "DEBUG:filelock:Lock 129337767927120 acquired on /root/.cache/huggingface/hub/.locks/models--Skywork--Skywork-o1-Open-PRM-Qwen-2.5-7B/39ebec613eb83f95e974925db2d9ec4dd6d3799885b546e8d1af9345cc722d4d.lock\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): cdn-lfs-us-1.hf.co:443\n",
      "DEBUG:urllib3.connectionpool:https://cdn-lfs-us-1.hf.co:443 \"GET /repos/e4/2f/e42f911f1cedf38d17c721855d1f6527b7b8b8be4d0ac7e5b1e1abdc6c3d4aae/39ebec613eb83f95e974925db2d9ec4dd6d3799885b546e8d1af9345cc722d4d?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1745594232&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NTU5NDIzMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2U0LzJmL2U0MmY5MTFmMWNlZGYzOGQxN2M3MjE4NTVkMWY2NTI3YjdiOGI4YmU0ZDBhYzdlNWIxZTFhYmRjNmMzZDRhYWUvMzllYmVjNjEzZWI4M2Y5NWU5NzQ5MjVkYjJkOWVjNGRkNmQzNzk5ODg1YjU0NmU4ZDFhZjkzNDVjYzcyMmQ0ZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=VluxQIiwVeHjB0XZbgft2h5VfO0gSThVsvSizyTXp2j-RJTQ~OrKn7P04IOLDwDn~pG~Q60W8sDDVdMGHgWBB5o9j93le2Lf-4ZFBl55HKndHv78BrMck7GXn1IUGrhD1aka5Gs6vsCYq8pgxooC9QFGDa835b1JOAiuk7lKMJI3VxMKaOw9znVZMEU2VEkaKLgtxFR2p27mz2y0XJw7BESXnkb2xnsMbIQOrl753l3bwl9iLCeyKIgOMYv9mOiGyaYVS-SzkY17d4yEmZCyK1Z3NiU1Vj0lgTPMvjsypMmoEy78NSxsqGNNEsyOkD9Xf4XrADoeq4iMAoh~tNI3bQ__&Key-Pair-Id=K24J24Z295AEI9 HTTP/1.1\" 200 15231308028\n",
      "DEBUG:filelock:Attempting to release lock 129337767927120 on /root/.cache/huggingface/hub/.locks/models--Skywork--Skywork-o1-Open-PRM-Qwen-2.5-7B/39ebec613eb83f95e974925db2d9ec4dd6d3799885b546e8d1af9345cc722d4d.lock\n",
      "DEBUG:filelock:Lock 129337767927120 released on /root/.cache/huggingface/hub/.locks/models--Skywork--Skywork-o1-Open-PRM-Qwen-2.5-7B/39ebec613eb83f95e974925db2d9ec4dd6d3799885b546e8d1af9345cc722d4d.lock\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Skywork/Skywork-o1-Open-PRM-Qwen-2.5-7B/resolve/main/model.safetensors HTTP/1.1\" 404 0\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--Skywork--Skywork-o1-Open-PRM-Qwen-2.5-7B/snapshots/c3f1e8065a3ca88cd65dd2f05665af0843d29e21/pytorch_model.bin\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "Will use torch_dtype=torch.bfloat16 as defined in model's config object\n",
      "Instantiating Qwen2ForRewardModel model under default dtype torch.bfloat16.\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/Skywork/Skywork-o1-Open-PRM-Qwen-2.5-7B HTTP/1.1\" 200 4521\n",
      "Attempting to create safetensors variant\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/Skywork/Skywork-o1-Open-PRM-Qwen-2.5-7B/commits/main HTTP/1.1\" 200 2826\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/Skywork/Skywork-o1-Open-PRM-Qwen-2.5-7B/discussions?p=0 HTTP/1.1\" 200 3810\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/Skywork/Skywork-o1-Open-PRM-Qwen-2.5-7B/commits/refs%2Fpr%2F5 HTTP/1.1\" 200 3791\n",
      "Safetensors PR exists\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Skywork/Skywork-o1-Open-PRM-Qwen-2.5-7B/resolve/refs%2Fpr%2F5/model.safetensors.index.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Skywork/Skywork-o1-Open-PRM-Qwen-2.5-7B/resolve/refs%2Fpr%2F5/model.safetensors HTTP/1.1\" 302 0\n",
      "DEBUG:filelock:Attempting to acquire lock 129346671100304 on /root/.cache/huggingface/hub/.locks/models--Skywork--Skywork-o1-Open-PRM-Qwen-2.5-7B/e6ca1a46c67cab786297803fe041a76c5479467c2c7bdce92d1a8e607dce16c3.lock\n",
      "DEBUG:filelock:Lock 129346671100304 acquired on /root/.cache/huggingface/hub/.locks/models--Skywork--Skywork-o1-Open-PRM-Qwen-2.5-7B/e6ca1a46c67cab786297803fe041a76c5479467c2c7bdce92d1a8e607dce16c3.lock\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): cdn-lfs-us-1.hf.co:443\n",
      "DEBUG:urllib3.connectionpool:https://cdn-lfs-us-1.hf.co:443 \"GET /repos/e4/2f/e42f911f1cedf38d17c721855d1f6527b7b8b8be4d0ac7e5b1e1abdc6c3d4aae/e6ca1a46c67cab786297803fe041a76c5479467c2c7bdce92d1a8e607dce16c3?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1745594339&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NTU5NDMzOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2U0LzJmL2U0MmY5MTFmMWNlZGYzOGQxN2M3MjE4NTVkMWY2NTI3YjdiOGI4YmU0ZDBhYzdlNWIxZTFhYmRjNmMzZDRhYWUvZTZjYTFhNDZjNjdjYWI3ODYyOTc4MDNmZTA0MWE3NmM1NDc5NDY3YzJjN2JkY2U5MmQxYThlNjA3ZGNlMTZjMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=ZtmWe9aPKxNAXursZuv3W69b54nl60S4RSREuKC9XzG0ENkIHnglB3VZEsgf0ZC6kDiXWKfQLJNfzq17xS7WJFEYJ80NEbpcqLpa9Am4qccO2LYgEXQ-vykAvdysWVhkJ-sj6nJHsc~DEAPxOT0r0WReOe0DHlxhL4b9KH79Qym8DJLmDhNB~MEa5Z-~1fjBFjUdyPmH6HcwYPR~QpYCTdYa9yVf~-x~i9XhdXTQHONdU~38JPOQlrIgG18BdOTOWp9sIOXJQwK3rHb5RARTT1GS2Y~x8AaRXUEK4Z~ycwTAfm7eN~Ug2FOEeYQcgnQ4xT1KSmBjpijCoXZD3Z-opg__&Key-Pair-Id=K24J24Z295AEI9 HTTP/1.1\" 200 15231279514\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSkywork/Skywork-o1-Open-PRM-Qwen-2.5-7B\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/tts_env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:564\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    562\u001b[39m     \u001b[38;5;28mcls\u001b[39m.register(config.\u001b[34m__class__\u001b[39m, model_class, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    563\u001b[39m     model_class = add_generation_mixin_to_remote_model(model_class)\n\u001b[32m--> \u001b[39m\u001b[32m564\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping.keys():\n\u001b[32m    568\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/tts_env/lib/python3.11/site-packages/transformers/modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/tts_env/lib/python3.11/site-packages/transformers/modeling_utils.py:4399\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4389\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4390\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   4392\u001b[39m     (\n\u001b[32m   4393\u001b[39m         model,\n\u001b[32m   4394\u001b[39m         missing_keys,\n\u001b[32m   4395\u001b[39m         unexpected_keys,\n\u001b[32m   4396\u001b[39m         mismatched_keys,\n\u001b[32m   4397\u001b[39m         offload_index,\n\u001b[32m   4398\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m4399\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4403\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4405\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4406\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4408\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4415\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4417\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   4418\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/tts_env/lib/python3.11/site-packages/transformers/modeling_utils.py:4638\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   4635\u001b[39m     original_checkpoint_keys = \u001b[38;5;28mlist\u001b[39m(state_dict.keys())\n\u001b[32m   4636\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4637\u001b[39m     original_checkpoint_keys = \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m-> \u001b[39m\u001b[32m4638\u001b[39m         \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmeta\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m.keys()\n\u001b[32m   4639\u001b[39m     )\n\u001b[32m   4641\u001b[39m \u001b[38;5;66;03m# Check if we are in a special state, i.e. loading from a state dict coming from a different architecture\u001b[39;00m\n\u001b[32m   4642\u001b[39m prefix = model.base_model_prefix\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/tts_env/lib/python3.11/site-packages/transformers/modeling_utils.py:556\u001b[39m, in \u001b[36mload_state_dict\u001b[39m\u001b[34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[39m\n\u001b[32m    549\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    550\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(checkpoint_file, \u001b[38;5;28mstr\u001b[39m)\n\u001b[32m    551\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m map_location != \u001b[33m\"\u001b[39m\u001b[33mmeta\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    552\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m version.parse(torch.__version__) >= version.parse(\u001b[33m\"\u001b[39m\u001b[33m2.1.0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    553\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m is_zipfile(checkpoint_file)\n\u001b[32m    554\u001b[39m     ):\n\u001b[32m    555\u001b[39m         extra_args = {\u001b[33m\"\u001b[39m\u001b[33mmmap\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mextra_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    563\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/tts_env/lib/python3.11/site-packages/torch/serialization.py:1090\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1088\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[32m   1089\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1090\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1091\u001b[39m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m                     \u001b[49m\u001b[43m_weights_only_unpickler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1094\u001b[39m \u001b[43m                     \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1096\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/tts_env/lib/python3.11/site-packages/torch/serialization.py:1525\u001b[39m, in \u001b[36m_load\u001b[39m\u001b[34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[39m\n\u001b[32m   1522\u001b[39m \u001b[38;5;66;03m# Needed for tensors where storage device and rebuild tensor device are\u001b[39;00m\n\u001b[32m   1523\u001b[39m \u001b[38;5;66;03m# not connected (wrapper subclasses and tensors rebuilt using numpy)\u001b[39;00m\n\u001b[32m   1524\u001b[39m torch._utils._thread_local_state.map_location = map_location\n\u001b[32m-> \u001b[39m\u001b[32m1525\u001b[39m result = \u001b[43munpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1526\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m torch._utils._thread_local_state.map_location\n\u001b[32m   1528\u001b[39m torch._utils._validate_loaded_sparse_tensors()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/tts_env/lib/python3.11/site-packages/torch/_weights_only_unpickler.py:342\u001b[39m, in \u001b[36mUnpickler.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    334\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    335\u001b[39m         \u001b[38;5;28mtype\u001b[39m(pid) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m\n\u001b[32m    336\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pid) > \u001b[32m0\u001b[39m\n\u001b[32m    337\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m torch.serialization._maybe_decode_ascii(pid[\u001b[32m0\u001b[39m]) != \u001b[33m\"\u001b[39m\u001b[33mstorage\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    338\u001b[39m     ):\n\u001b[32m    339\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    340\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOnly persistent_load of storage is allowed, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpid[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m     \u001b[38;5;28mself\u001b[39m.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpersistent_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m key[\u001b[32m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [BINGET[\u001b[32m0\u001b[39m], LONG_BINGET[\u001b[32m0\u001b[39m]]:\n\u001b[32m    344\u001b[39m     idx = (read(\u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m key[\u001b[32m0\u001b[39m] == BINGET[\u001b[32m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m unpack(\u001b[33m\"\u001b[39m\u001b[33m<I\u001b[39m\u001b[33m\"\u001b[39m, read(\u001b[32m4\u001b[39m)))[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/tts_env/lib/python3.11/site-packages/torch/serialization.py:1492\u001b[39m, in \u001b[36m_load.<locals>.persistent_load\u001b[39m\u001b[34m(saved_id)\u001b[39m\n\u001b[32m   1490\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1491\u001b[39m     nbytes = numel * torch._utils._element_size(dtype)\n\u001b[32m-> \u001b[39m\u001b[32m1492\u001b[39m     typed_storage = \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1494\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/tts_env/lib/python3.11/site-packages/torch/serialization.py:1457\u001b[39m, in \u001b[36m_load.<locals>.load_tensor\u001b[39m\u001b[34m(dtype, numel, key, location)\u001b[39m\n\u001b[32m   1455\u001b[39m     storage = overall_storage[storage_offset:storage_offset + numel]\n\u001b[32m   1456\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1457\u001b[39m     storage = \u001b[43mzip_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m)\u001b[49m._typed_storage()._untyped_storage\n\u001b[32m   1458\u001b[39m \u001b[38;5;66;03m# swap here if byteswapping is needed\u001b[39;00m\n\u001b[32m   1459\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m byteorderdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\n",
    "    \"Skywork/Skywork-o1-Open-PRM-Qwen-2.5-7B\",\n",
    "    trust_remote_code=True,\n",
    "    use_safetensors=False,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7ec416-2e6e-495a-afa4-d61c86d33017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954457b1-e3f9-4587-90a5-f8f9ed41e495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaac169d-cfa7-47d7-ab49-fc249d4ce6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2390f04b-5dd1-437d-9892-b8cd0dd16d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Skywork/Skywork-o1-Open-PRM-Qwen-2.5-7B/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Skywork/Skywork-o1-Open-PRM-Qwen-2.5-7B/resolve/main/configuration_qwen2_rm.py HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers_modules.Skywork.Skywork-o1-Open-PRM-Qwen-2.5-7B.c3f1e8065a3ca88cd65dd2f05665af0843d29e21.configuration_qwen2_rm.Qwen2RMConfig'> for this kind of AutoModel: AutoModelForTokenClassification.\nModel type should be one of AlbertConfig, BertConfig, BigBirdConfig, BioGptConfig, BloomConfig, BrosConfig, CamembertConfig, CanineConfig, ConvBertConfig, Data2VecTextConfig, DebertaConfig, DebertaV2Config, DiffLlamaConfig, DistilBertConfig, ElectraConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FNetConfig, FunnelConfig, GemmaConfig, Gemma2Config, GlmConfig, Glm4Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, HeliumConfig, IBertConfig, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LiltConfig, LlamaConfig, LongformerConfig, LukeConfig, MarkupLMConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MobileBertConfig, ModernBertConfig, MPNetConfig, MptConfig, MraConfig, MT5Config, NemotronConfig, NezhaConfig, NystromformerConfig, PersimmonConfig, PhiConfig, Phi3Config, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, SqueezeBertConfig, StableLmConfig, Starcoder2Config, T5Config, UMT5Config, XLMConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YosoConfig.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mAutoModelForTokenClassification\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprm_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m tok   = AutoTokenizer.from_pretrained(prm_model_path, trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/tts_env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:574\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m    571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class.from_pretrained(\n\u001b[32m    572\u001b[39m         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n\u001b[32m    573\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Unrecognized configuration class <class 'transformers_modules.Skywork.Skywork-o1-Open-PRM-Qwen-2.5-7B.c3f1e8065a3ca88cd65dd2f05665af0843d29e21.configuration_qwen2_rm.Qwen2RMConfig'> for this kind of AutoModel: AutoModelForTokenClassification.\nModel type should be one of AlbertConfig, BertConfig, BigBirdConfig, BioGptConfig, BloomConfig, BrosConfig, CamembertConfig, CanineConfig, ConvBertConfig, Data2VecTextConfig, DebertaConfig, DebertaV2Config, DiffLlamaConfig, DistilBertConfig, ElectraConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FNetConfig, FunnelConfig, GemmaConfig, Gemma2Config, GlmConfig, Glm4Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, HeliumConfig, IBertConfig, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LiltConfig, LlamaConfig, LongformerConfig, LukeConfig, MarkupLMConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MobileBertConfig, ModernBertConfig, MPNetConfig, MptConfig, MraConfig, MT5Config, NemotronConfig, NezhaConfig, NystromformerConfig, PersimmonConfig, PhiConfig, Phi3Config, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, SqueezeBertConfig, StableLmConfig, Starcoder2Config, T5Config, UMT5Config, XLMConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YosoConfig."
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(prm_model_path, trust_remote_code=True)\n",
    "tok   = AutoTokenizer.from_pretrained(prm_model_path, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702a1bea-cb78-42b6-967f-0e690309395d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612081eb-b5f6-4b80-aced-777c07939d05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae67b38a-5ded-4d81-8c02-b918f97d9a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/tts_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sal.config import Config\n",
    "from sal.utils.parser import H4ArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "688079f2-0da0-4385-87fc-c458ebd0cd5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Some specified arguments are not used by the HfArgumentParser: ['-f', '/root/.local/share/jupyter/runtime/kernel-4c80ea62-7cc6-4584-a147-ab27b3e92559.json']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m parser = H4ArgumentParser(Config)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m config = \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/projects/tts_experiments/src/sal/utils/parser.py:113\u001b[39m, in \u001b[36mH4ArgumentParser.parse\u001b[39m\u001b[34m(self, allow_extra_keys)\u001b[39m\n\u001b[32m    108\u001b[39m     output = \u001b[38;5;28mself\u001b[39m.parse_yaml_and_args(\n\u001b[32m    109\u001b[39m         os.path.abspath(sys.argv[\u001b[32m1\u001b[39m]), sys.argv[\u001b[32m2\u001b[39m:]\n\u001b[32m    110\u001b[39m     )\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# parse command line args only\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_args_into_dataclasses\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(output) == \u001b[32m1\u001b[39m:\n\u001b[32m    116\u001b[39m     output = output[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/tts_env/lib/python3.11/site-packages/transformers/hf_argparser.py:366\u001b[39m, in \u001b[36mHfArgumentParser.parse_args_into_dataclasses\u001b[39m\u001b[34m(self, args, return_remaining_strings, look_for_args_file, args_filename, args_file_flag)\u001b[39m\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    365\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m remaining_args:\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSome specified arguments are not used by the HfArgumentParser: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mremaining_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    368\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (*outputs,)\n",
      "\u001b[31mValueError\u001b[39m: Some specified arguments are not used by the HfArgumentParser: ['-f', '/root/.local/share/jupyter/runtime/kernel-4c80ea62-7cc6-4584-a147-ab27b3e92559.json']"
     ]
    }
   ],
   "source": [
    "parser = H4ArgumentParser(Config)\n",
    "config = parser.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d4a3dc5-cee9-4130-8d6b-5c0362df9c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"/workspace/projects/tts_experiments/logs/dss/20250310_055859_062566_cluster_logs/cluster_log.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "144aadd1-bfe4-4afa-96c9-beee0ee6663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_id = list(set(df['problem_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ea74518-58e1-495f-97bb-8d0d51bf7d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comp_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a16b000e-a148-48b0-a38f-af77c98ccfab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['problem_id']==\"test/intermediate_algebra/754.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3d3fd62-b918-40de-a114-c5a87bd65e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['problem_id']==\"test/algebra/2789.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bc6ffe1-911b-4ff7-a2ce-d36e0ec87e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test/number_theory/342.json  :  39\n",
      "test/prealgebra/996.json  :  39\n",
      "test/algebra/2789.json  :  39\n",
      "test/counting_and_probability/761.json  :  39\n",
      "test/geometry/226.json  :  39\n",
      "test/prealgebra/1113.json  :  40\n",
      "test/algebra/722.json  :  40\n",
      "test/number_theory/612.json  :  39\n",
      "test/intermediate_algebra/2046.json  :  39\n",
      "test/intermediate_algebra/754.json  :  17\n",
      "test/prealgebra/307.json  :  39\n",
      "test/number_theory/466.json  :  39\n",
      "test/algebra/24.json  :  39\n"
     ]
    }
   ],
   "source": [
    "for item in comp_id:\n",
    "    print(item,\" : \",len(df[df['problem_id']==item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5a5ac6d-b388-4c87-af8a-1c1cb1b71d37",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'level'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/tts_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'level'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlevel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.value_counts()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/tts_env/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/tts_env/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'level'"
     ]
    }
   ],
   "source": [
    "df['level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9839c354-e3b8-4b3f-83aa-3147e3427696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test/number_theory/342.json',\n",
       " 'test/prealgebra/996.json',\n",
       " 'test/algebra/2789.json',\n",
       " 'test/counting_and_probability/761.json',\n",
       " 'test/geometry/226.json',\n",
       " 'test/prealgebra/1113.json',\n",
       " 'test/algebra/722.json',\n",
       " 'test/number_theory/612.json',\n",
       " 'test/intermediate_algebra/2046.json',\n",
       " 'test/intermediate_algebra/754.json',\n",
       " 'test/prealgebra/307.json',\n",
       " 'test/number_theory/466.json',\n",
       " 'test/algebra/24.json']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc9ad78f-0899-4aa3-b0e2-6305fb4d7354",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa= ['test/number_theory/342.json',\n",
    " 'test/prealgebra/996.json',\n",
    " 'test/algebra/2789.json',\n",
    " 'test/counting_and_probability/761.json',\n",
    " 'test/geometry/226.json',\n",
    " 'test/prealgebra/1113.json',\n",
    " 'test/algebra/722.json',\n",
    " 'test/number_theory/612.json',\n",
    " 'test/intermediate_algebra/2046.json',\n",
    " 'test/prealgebra/307.json',\n",
    " 'test/number_theory/466.json',\n",
    " 'test/algebra/24.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d846f1-f2bc-403c-91cb-f03252995d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(config)\n",
    "df = pd.DataFrame(dataset)\n",
    "df = df.groupby('level', group_keys=False).apply(lambda x: x.sample(n=10, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66d7a59f-f162-4e7e-9e81-74b2a9410373",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[1,2,4,5,6,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95a81864-8d2e-4171-81d8-0c532883134d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m7\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "a[0,5,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb348a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8de2c425-126d-48dc-8650-f8de9ee548dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/tts_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-23 13:00:14,730\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "/workspace/tts_env/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM,SamplingParams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a46f907-9c3b-4601-841b-c5114962fc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-23 13:00:37 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='Qwen/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=Qwen/Qwen2.5-1.5B-Instruct, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 05-23 13:00:44 model_runner.py:1060] Starting to load model Qwen/Qwen2.5-1.5B-Instruct...\n",
      "INFO 05-23 13:00:44 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-23 13:00:54 weight_utils.py:288] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.47it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.46it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-23 13:00:54 model_runner.py:1071] Loading model weights took 2.8875 GB\n",
      "INFO 05-23 13:00:55 gpu_executor.py:122] # GPU blocks: 96310, # CPU blocks: 9362\n",
      "INFO 05-23 13:00:55 gpu_executor.py:126] Maximum concurrency for 32768 tokens per request: 47.03x\n",
      "INFO 05-23 13:01:00 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-23 13:01:00 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-23 13:01:12 model_runner.py:1530] Graph capturing finished in 12 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(\n",
    "        model=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "        gpu_memory_utilization=0.5,\n",
    "        enable_prefix_caching=True,\n",
    "        seed=42,\n",
    "        tensor_parallel_size=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c01ba86-3188-4110-94a1-12971b792a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32768"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.llm_engine.model_config.max_model_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5109b2d6-09d5-4afd-8c6e-2856d5d98903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dac2c75-90ce-49ba-817d-75671ac7a6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer max length: 131072\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"RLHFlow/Llama3.1-8B-PRM-Deepseek-Data\")\n",
    "print(\"Tokenizer max length:\", tokenizer.model_max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d285ebc-95b2-4353-8ca1-3cf51af65e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166bf77f-9227-4e59-8e56-8523150e01ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d6b33-28a7-4cd0-bf8d-e46804a4decd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759ff446-9379-4abc-b635-5a8d1805d5d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4dd7d280-c586-4711-9708-4eac80348b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  9.39it/s, est. speed input: 75.58 toks/s, output: 179.48 toks/s]\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    max_tokens=20,\n",
    "    logprobs=1,  # Request top 5 logprobs for each token\n",
    "    stop=[\",\"],\n",
    "    include_stop_str_in_output=True,\n",
    "    n=1\n",
    ")\n",
    "\n",
    "# Run generation\n",
    "prompts = [\"Jello,can you use , \"]\n",
    "outputs = llm.generate(prompts, sampling_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e6cf026-11cd-4f90-bf60-69e211f21514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.9, top_k=-1, min_p=0.0, seed=None, stop=[','], stop_token_ids=[], include_stop_str_in_output=True, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=1, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=None\n"
     ]
    }
   ],
   "source": [
    "print(sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf2ea981-5435-4db1-9cd2-34aed35eb02f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 ,'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf733dc2-d877-44f8-9283-0a1a88cfd3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionOutput(index=0, text='2 ,', token_ids=(17, 1154), cumulative_logprob=-5.110428214073181, logprobs=[{17: Logprob(logprob=-1.3613923788070679, rank=1, decoded_token='2')}, {1154: Logprob(logprob=-3.7490358352661133, rank=9, decoded_token=' ,'), 87: Logprob(logprob=-1.0674716234207153, rank=1, decoded_token='x')}], finish_reason=stop, stop_reason=,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4489fe31-ed10-4e78-a370-ff2dc11ee617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.48586905002594"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].outputs[0].cumulative_logprob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e74ade11-485e-43ed-82a1-f6bb1a18db24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{21: Logprob(logprob=-2.612788677215576, rank=6, decoded_token='6'),\n",
       "  17: Logprob(logprob=-1.3613923788070679, rank=1, decoded_token='2')},\n",
       " {11: Logprob(logprob=-0.8730803728103638, rank=1, decoded_token=',')}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].outputs[0].logprobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b32dd70-f7a0-44e9-acbf-cd4286183fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.268758475780487"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-0.34293365478515625-0.6312437057495117-1.2106025218963623-1.0636543035507202-1.5811859369277954-0.5633963346481323-0.8757420182228088"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92007d2a-df8e-482e-9288-b394cbe97309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.827867925167084"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1.5238444805145264-2.000767230987549-0.6587409973144531-2.2116239070892334-0.32903236150741577-2.1038589477539062"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7c12979-989d-46d5-b598-f2d505782f66",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for +: 'int' and 'dict'"
     ]
    }
   ],
   "source": [
    "outputs[0].outputs[0].logprobs\n",
    "-8.827867925167084"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8664cbf8-7bc1-4c8f-9583-1dcda195fa79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1/-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef99cf94-fa97-48c4-93d7-d5a15ae034f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "            total_prob = np.exp(total_logprob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tts_env",
   "language": "python",
   "name": "tts_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
